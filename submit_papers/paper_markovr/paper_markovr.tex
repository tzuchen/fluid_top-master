\documentclass{article}
\usepackage{graphicx}
%\usepackage{graphicx,psfrag}
\usepackage{graphics}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\title{Markov chain reduction}
\author{T. Liang and M. West}
\date{\today}
\graphicspath{{figures/}}
\usepackage[margin=2.5cm]{geometry}

\newtheorem{definition}{Definition}
\newtheorem{conjecture}{Conjecture}
\newtheorem{example}{Example}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% todo box

\usepackage{color}
\newcommand{\todo}[1]{\noindent\framebox{\begin{minipage}{0.97\columnwidth}\color{red}#1\end{minipage}}}
\newcommand{\mwtodo}[1]{\noindent\framebox{\begin{minipage}{0.97\columnwidth}\color{blue}#1\end{minipage}}}

% temporary disable todo
%\renewcommand{\todo}[1]{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

Markov chain question. I think the ``answer'' is related to
the matrix:
\begin{equation}
M =  [ A^{n-1}x_0, A^{n-2}x_0, ... , Ax_0, x_0]    
\end{equation}
$A \in \mathbb{R}^{n\times n}$. $M$ is just the controllability matrix of $[A,x_0]$  ($x_0$ is the ``$B$''
matrix in control language)
if M is not full rank, then some of the states can be lumped together.
The reduced Markov chain has identical total variation distance as the
original Markov chain.

Of course this statement is embarrassing. We need to evolve the system
for n iterations to know if it is reducible. All of the interesting
phenomenon like cutoff probably happens within n iterations.


I agree with your intuition about the controllability matrix. It's always seemed to me that the idea behind balanced truncation should be the right way to think about the Markov chain problem as well, given that MC is just a linear system anyway. I think there are two main problems: (1) figuring out exactly what the right statement is in the MC setting, what the right norm is, the right set of initial conditions or distributions, etc, (2) designing algorithms that do not require any operations actually using the Markov matrix A at all, given that it is typically much too big to form and do SVDs on, etc.

Of course one could ignore (2) initially until (1) was better understood. It seems to me that for transient model reduction the choice of initial conditions (or sets of initial distributions) will be key, as presumably the best model reduction will be different for very different initial conditions. I'm not sure how to approach this. What are your thoughts?


I think for a given $x_0$, when the svd of controllability matrix of $[A,x_0]$ is calculated, the best reduction is also known.

Take this simple example: 

\begin{example}

We divide $[0,1]^2$ in to n-grids, $\sqrt{n}$ by $\sqrt{n}$
and the Markov matrix is simply a diffusion operator to represent
$x_{ij} = 1/5 \times (x_{i,j}+x_{i+1,j}+x_{i-1,j}+x_{i,j+1}+x_{i,j-1})$, here
i, j are the 2D coordinates
$x^0$ is $[0,...,0,1,0,...,0]$, where 1 appears at $n/2$ state. This is a
point distribution at $[0.5,0.5]$.

Let $p=\sqrt{n}/2$, $x^0$ has $x_{p,p}=1$, others are zeros.

We evolve the above system by $x^{k+1} = A^T x^k$, clearly we shall see
the 4 states $x_{p+1,p}$, $x_{p-1,p}$, $x_{p,p+1}$, $x_{p,p-1}$,  are always
the same from iteration 0 to $k$ (and to infinity) because of symmetry.
Thus M is not full rank and these 4 states can be lumped together to a
single state with no approximation.

By finding the symmetric states, we can reduce the number of states to
around $n/8$, so I think the rank of $M$ is probably $n/8$. The svd of $M$ may
even suggest that the number of states can be reduced to $\sqrt{n}/2$
because this is truly a 1-D diffusion in r-direction.

\end{example}


 I have not tried this example by myself, but I can image only the last step is problematic. The coordinate
 change suggested by svd may make the matrix not Markov anymore.

Again, the example is not practically useful, unless we have some fast
 way to calculate svd of $M$ (your point two). For an arbitrary A and
 arbitrary $x^0$, this is of course hopeless. But for the A from some
 symmetric structure/graph and $x^0$ doesn't destroy the symmetry
 completely, like the random walk and riffle shuffle problems, there
 might be some hope.

 For example, we already know the rank of $M$ in $n-d$ random walk problem
 is n, though the size of $M$ is $2^n$ by $2^n$. The proof can be done by
 enumerate all the symmetric states from $x^0$ to $x^k$. But there might be
 some sophisticated/general ways to do it?

Yes, this is certainly the key hard problem, I think. I really wonder if it's possible to approximate this somehow using just samples from the original Markov chain, or something, to slowly build up or calculate parts of the reduced chain. Not that I know how!

\begin{example}
random walk on a $3$-dimensional cube. We have
\begin{equation}
A = 
\left[  \begin{array}{cccccccc}
        a & a & a & a & 0 & 0 & 0 & 0 \\ 
        a & a & 0 & 0 & a & a & 0 & 0 \\
        a & 0 & a & 0 & a & 0 & a & 0 \\
        a & 0 & 0 & a & 0 & a & a & 0 \\
        0 & a & a & 0 & a & 0 & 0 & a \\
        0 & a & 0 & a & 0 & a & 0 & a \\
        0 & 0 & a & a & 0 & 0 & a & a \\
        0 & 0 & 0 & 0 & a & a & a & a         
		\end{array} \right]
\end{equation}
\end{example}
where $a=\frac{1}{4}$.
Let $x^0=[1,0,0,0,0,0,0,0]^T$, we have rank of $M$ equals $4$. There exists a $4 \times 4$ reduced Markov chain $\bar{A}$ representing $A$ without approximation.  


Let 
\begin{equation}
P = 
\left[  \begin{array}{cccccccc}
        1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 
        0 & 1 & 1 & 1 & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & 1 & 1 & 1 & 0 \\
        0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 
		\end{array} \right],
Q = 
\left[  \begin{array}{cccccccc}
        1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\ 
        0 & \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & \frac{1}{3} & \frac{1}{3} & \frac{1}{3} & 0 \\
        0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 
		\end{array} \right]
\end{equation}


We have
\begin{equation}
 \bar{A} = Q A P^T
\end{equation}

The reduced Markov matrix $\bar{A}$ is thus

\begin{equation}
\bar{A} = 
\left[  \begin{array}{cccc}
        \frac{1}{4} & \frac{3}{4} & 0           & 0            \\ 
        \frac{1}{4} & \frac{1}{4} & \frac{1}{2} & 0            \\
        0           & \frac{1}{2} & \frac{1}{4} & \frac{1}{4}  \\
        0           & 0           & \frac{3}{4} & \frac{1}{4}  \\ 
		\end{array} \right]
\end{equation}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
 
\end{abstract}



% References
 %\bibliographystyle{plain}
 \bibliographystyle{abbrv}
\bibliography{../mixingbib}

\end{document}
